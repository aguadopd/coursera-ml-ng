#+author: Pablo Aguado
#+title: Notas del curso Machine Learning de Andrew Ng en Coursera
#+description: Mis notas.

#+STARTUP: indent content align entitiespretty


* Info

- https://www.coursera.org/learn/machine-learning
- [[https://www.coursera.org/learn/machine-learning/discussions/all/threads/v2YppY8FEeWIeBJxvl1elQ][Important notes for new ML students]]
  - Hay más /test cases/ en los Recursos del curso.
  - *Hay que usar Octave > 4.0.0*
  - [[https://learner.coursera.help/hc/en-us/articles/209818863-Coursera-Honor-Code][Cousera Honor Code]]

* Ideas

Ideas mías a lo largo del curso.

1. Probar [[https://github.com/google-research/google-research/blob/master/automl_zero/README.md][AutoML-Zero]].
2. Buscar clusters en espacios transformados y muy transformados. Ej: Fourier, Fourier de Fourier, Cepstrum...


* Semana 1

Intro, regresión lineal, repaso de Álgebra.


- [[https://www.coursera.org/learn/machine-learning/discussions/weeks/1/threads/hAp4LT1SEeaL_xIEq4QdBw][FAQ de la semana 1]]

** Introduction
*** Video: Welcome

*** Video: What is Machine Learning

- Los algoritmos más importantes son el aprendizaje supervisado y el aprendizaje no supervisado. Es esta además la clasificación más general de algoritmos.
  - Otros son el aprendizaje por refuerzo y los sistemas de recomendación.
- Hay que aprender las herramientas, pero *es muy importante saber cómo y cuándo usarlas*.
- Sea una máquina que debe hacer una tarea T, con un desempeño P y que la exponemos a experiencias (instancias) E de esa tarea T. Se dice que la computadora aprende si su desempeño P en la tarea T /aumenta proporcionalmente a la cantidad de experiencias E/.
- Otra definición de aprendizaje automático es la capacidad (de la computadora) de aprender a resolver problemas para los que no fue programada. ~

*** Reading: What is Machine Learning?

*** Video: Supervised Learning

- En el aprendizaje supervisado, le mostramos al programa ejemplos de entradas y sus correspondientes salidas/respuestas correctas. Ya sabemos cómo son las respuestas corectas; tenemos la idea de que hay una relación entre las entradas y las salidas. Dado un conjunto de entradas y salidas, intentamos obtener un modelo que permita predecir/inferir las salidas a nuevos datos de entrada.
- Los problemas de aprendizaje supervisado se clasifican en problemas de regresión y de clasificación:
  - Problema de *regresión* si el conjunto imagen es continuo. La salida es una variable numérica.
  - Problema de *clasificación* si el conjunto imagen es discreto. La salida es una variable categórica.
- Los algoritmos de Máquinas de Vector Soporte permiten /*infinitos*/ valores de entrada.

****** TODO Leer https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression

****** TODO Leer https://datascience.stackexchange.com/questions/25298/how-to-know-when-to-treat-a-problem-as-a-classification-task-or-a-regression-tas

*** Video: Unsupervised Learning

- En el aprendizaje no supervisado, le damos datos al programa con la intención de encontrar estructuras subyacentes, patrones.
- Un ejemplo típico es el /clustering/ o agrupamiento de datos.
- En el ejemplo de sonido Cocktail Party, según [[https://www.coursera.org/learn/machine-learning/discussions/weeks/1/threads/hAp4LT1SEeaL_xIEq4QdBw][FAQ de la semana 1]], lo que usan es /Principal Component Analysis, PCA, a mathematical trick that takes two sets of correlated data, and returns two new sets of data that are not correlated./ No lo había visto así antes, creo...

** Model and cost function

Vemos la regresión lineal como primer algoritmo de aprendizaje supervisado.

*** Video: Model representation

Un poco de nomenclatura:

- $m$: cantidad de ejemplos de entrenamiento.
- $\vec{x}$: entradas / descriptores / /features/
- $\vec{y}$: salidas. $\hat{\vec{y}}$ son las salidas estimadas.
- $h_\theta$: función de hipótesis, de estimación. Tiene parámetros $\vec{\theta}$. Entonces tenemos que \( \hat{y}^{(i)} = h_\theta(x^{(i)}) = h(x,\theta) \)
- $x^{(i)}$: entrada $i$-ésima del vector de entradas, con índices empezando en 1.
  - $(x^{(i)},y^{(i)})$ es un ejemplo de entrenamiento.
- Para regresión lineal de una variable tenemos entonces 
\[ \hat{y}^{(i)} = h_\theta(x^{(i)}) = \theta_0 + \theta_1 * x^{(i)} \]

*** Reading: Model representation

- $X$: el espacio de los valores de entrada.
- $Y$: el espacio de los valores de salida.
- El objetivo del aprendizaje supervisado es encontrar una función $h: X \rightarrow Y$ que sea buena prediciendo salidas a partir de entradas.
 
*** Video: Cost function

Formalizamos el problema del aprendizaje como la minimización de una función de costo $J(\vec{\theta})$. La función de costo habitual y recomendada para problemas de regresión lineal es el *error cuadrático medio* ([[https://en.wikipedia.org/wiki/Mean_squared_error][/Mean Squared Error/]] o /Mean Squared Deviation/).

Para un predictor como lo es $h_\theta$, el MSE se define como
\[ MSE = \frac{1}{N} (\sum_{1}^{N}Y_i - \hat{Y}_i )^2\]

En nuestro caso vamos a definir a la función de costo para este problema de regresión lineal univariable como

\[ J(\theta_0 , \theta_1) = \frac{1}{2m} \sum_{i=1}^m( h_\theta(x^{(i)}) - y^{(i)} )^2  \]
\[ J(\theta_0 , \theta_1) =  \frac{1}{2m} \sum_{i=1}^m( \theta_0 + \theta_1 * x^{(i)} - y^{(i)} )^2 \]

- El factor $1/2$ es para ahorrar cálculos, puesto que en redes neuronales al hacer /backpropagation/ o /gradient descent/ hay que derivar esta función de error y entonces con este $1/2$ simplificamos el $2$ de la derivada del cuadrado.

La optimización es entonces encontrar los parámetros $\theta$ que minimizan la función de costo:
\[ \underset{\theta_0 , \theta_1}{\text{min}}  J(\theta_0 , \theta_1)\]

------

/En las notas del curso encontramos la forma matricial, que luego usamos para hacer descenso por el gradiente de forma matricial. Lo pongo acá por completitud/.

MSE: \[ J(\theta) = \frac{1}{2m} (X\times\theta-Y)^T(X\times\theta-Y)  \]

El producto implica la sumatoria y el cuadrado elemento a elemento.

------

*** Reading: Cost function

*** Video: Cost function intuition I

*** Reading: Cost function intuition I

*** Video: Cost function intuition II

*** Reading: Cost function intuition II

De [[https://es.wikipedia.org/wiki/Isol%C3%ADnea][isolíneas / curvas de nivel]].


** Parameter learning

*** Video: Gradient descent

El descenso por el gradiente es un algoritmo de optimización que vamos a usar (entre otras cosas) para minimizar la función de costo.

Hacer \[ \vec{\theta}[n+1] := \vec{\theta}[n] - \alpha \frac{\delta J(\vec{\theta})}{\delta\theta}  \]

(expresado de otra manera)

\[ {\theta}_j[n+1] := {\theta}_j[n] - \alpha \frac{\delta J(\vec{\theta})}{\delta\theta}  \]

Hasta la convergencia de \(\vec{\theta}\), equivalente a la convergencia de \(J(\vec{\theta})\):

\[  \vec{\theta}[n] - \vec{\theta}[n-1] < \vec{\epsilon} \]
\[ J(\vec{\theta}[n]) - J(\vec{\theta}[n-1])  < \epsilon  \]

- Nomenclatura: usamos $:=$ como operador de asignación.
- $\alpha$ es la tasa de aprendizaje o /learning rate/ del algoritmo.

Para calcular la derivada hacemos derivadas parciales. Actualizamos los parámetros simultáneamente en cada paso. Si actualizamos de a uno y recalculamos estamos haciendo otro algoritmo, que probablemente también converja pero es distinto.

Cuando la función de costo es el error cuadrático medio (/MSE/), la fórmula de actualización queda:

\[ \theta_j[n+1] := {\theta}_j[n] - \frac{\alpha}{m}  \sum_{i=1}^m( h_\theta(x^{(i)}) - y^{(i)} ) x_j^{(i)}  \]

- El primer termino de la sumatoria es la magnitud y dirección del error.
- El segundo término de la sumatoria es la sensibilidad de J respecto al parámetro, y resulta ser igual a la magnitud del descriptor asociado a ese parámetro.

-----

- [[https://www.youtube.com/watch?v=WnqQrPNYz5Q][Un video de /gradient descent/ sugerido en las notas del curso]].

*** Reading: Gradient descent

*** Video: Gradient descent intuition

- Si $\alpha$ es muy grande, el algoritmo puede oscilar o incluso diverger.
- Si $\alpha$ es muy chica, puede tardar mucho en converger.
- Con $\alpha$ fija, los "pasos" que da el algoritmo son cada vez más chicos a medida que la función de costo se aproxima a un mínimo local.

*** Reading: Gradient descent intuition

*** Video: Gradient descent for linear regression

Dice Andrew cerca del minuto 4:40:

#+begin_quote
But, it turns out that that the cost function for
linear regression is always going to be a bow shaped function like this.
The technical term for this is that this is called a convex function.
#+end_quote

¿Por qué?

- La función de costo $J(\vec{\theta})$ es el error cuadrático medio (MSE).
- El MSE es cuadrático respecto a los parámetros siempre y cuando estos sean lineales, de grado 1. *La función de hipótesis debe ser lineal respecto a los parámetros para que la función de costo sea cuadrática*.
  - Sea por ejemplo \[ h(x,y) =  a.x^2 + b.y^2 - c.x^2 y^2 \]. Esta función tiene más de un mínimo.

[[file:imgs/001-01-nolineal.gif]]

  - Su MSE quedaría algo como \[ x^4 + 2 x^2 y^2 - 2 x^4 y^2 + y^4 - 2 x^2 y^4 + x^4 y^4  \] (sólo [[https://www.wolframalpha.com/input/?i=%28x%5E2+%2B+y%5E2+-+x%5E2y%5E2%29%5E2][la elevé al cuadrado]])

[[file:imgs/001-02-nolineal-cuadrado.gif]]

-----------------

Hay otras formas de estimar los parámetros (regresores). Una de ellas es el método de los mínimos cuadrados ([[https://en.wikipedia.org/wiki/Ordinary_least_squares][/Ordinary Least Squares]]/). El descenso por el gradiente es más fácil de computar que OLS, en el caso de datasets grandes.

En realidad todo lo que vimos es descenso por el gradiente por lotes, o */batch gradient descent/*, que es cuando la función de costo se optimiza usando todas las entradas disponibles. Esto es costoso.



****** TODO Leer más de [[https://en.wikipedia.org/wiki/Linear_regression][regresión lineal]]



**** Regresión lineal



*** Reading: Gradient descent for linear regression

** Linear Algebra review

*** Video: Matrix vector multiplication

- Más adelante vamos a ver por qué es mejor vectorizar calculos en lugar de iterar.
- Hace un truco interesante que es incluir a la ordenada al origen dentro del vector de parámetros ---en realidad está bien, es un parámetr calculado---, y luego introduce una columna de \(1\)s en la matriz de entradas.
  - La alternativa es sumar la columna aparte. $A*X + B$

*** Video: Matrix matrix multiplication

- Acá hace el mismo truco pero para hacer varias predicciones a la vez: usa varios modelos y varias entradas.

*** Video: Inverse and transpose

Interesante:
#+BEGIN_QUOTE
But the intuition if you want is that you can think of matrices as not have an inverse that is somehow too close to zero in some sense.
#+END_QUOTE

- Las matrices que no tienen inversa son matrices /singulares/ o /degeneradas/.
  - Asumo que se refiere a matrices cuadradas, que podrían tener inversa.

* Semana 2

** Environment setup instructions

** Multivariate linear regression

*** Video: Multivariate linear regression

En la regresión lineal multivariable o regresión lineal múltiple, tenemos varios valores de entrada o descriptores. Para tener una notación más compacta y conveniente, vamos a definir:
- $\theta_0=1$ ;
- $n$ es la cantidad de entradas, descriptores;
- vamos a usar $\vec{\theta}$ con índice $0$;
- y $\vec{\theta}_j^{(i)}$ es el elemento j-ésimo del ejemplo i-ésimo.

Entonces $\vec{\theta}$ tiene $n+1$ elementos y  \[ \vec{\theta} = 1 + \theta_1 + \theta_2 + \dots + \theta_n  \]


Y luego \[ \vec{h_\theta}(\vec{x}) = \vec{\theta}^T \cdot \vec{x}  = \vec{x}^T \cdot \vec{\theta}  \]

- Intuición para el ejemplo de estimar el precio de un inmueble: $\theta_0$ es el precio base.

------

/En [[*Video: Normal equation]] se introduce notación matricial que luego en el ejercicio 1 usamos para expresar todo de forma vectorizada. Dejo todo acá para más completitud/.

\[ \hat{Y}(\theta,X) = X \theta  ]\

------

*** Reading: multiple features

*** Video: Gradient descent for multiple features

La regla de actualización era:

 \[ \vec{\theta}[n+1] := \vec{\theta}[n] - \alpha \frac{\delta J(\vec{\theta})}{\delta\theta}  \]

Y para cuando la función de costo es el error cuadrático medio (MSE), queda (para actualización con *todos los $m$ ejemplos*):

\[ \theta_j[n+1] := {\theta}_j[n] - \frac{\alpha}{m}  \sum_{i=1}^m( h_\theta(x^{(i)}) - y^{(i)} ) x_j^{(i)}  \]

- Puedo ver el factor de avance luego de $\alpha$ como el aporte al error medio que hizo el descriptor $x_j$ .
  - El producto vectorial y la resta son el error medio para ese vector de entrada.
  - El factor $x_j$ es el aporte de ese elemento, en esa dirección.
    - La dirección final es la suma vectorial de los elementos.

-------

/La versión vectorizada/matricial del algoritmo está en las notas del curso y después la usamos en el ejercicio de programación 1. La dejo acá por completitud/.

\[ \theta_{n \times 1}[i+1] = \theta_{n \times 1}[i] - \frac{\alpha}{m} X_{m \times n}^T (X_{m \times n} \theta_{n \times 1} - Y_{m \times 1} )_{m \times 1} \]
\[ \theta_{}[i+1] = \theta[i] - \frac{\alpha}{m} X^T (X \theta - Y) \]

-----------

**** TODO EL ERROR ES MAYPR CUANDO HAY CORRELACIÓN ENTRE DESCRIPTORES Y PARÁMETROS.

*** Reading: Gradient descent for multiple features

*** Video: Gradient descent in practice I - Feature scaling

- Al parecer, el algoritmo de descenso por el gradiente converge *bastante más rápidamente* si los descriptores están en el mismo orden de magnitud.
  - Andrew propone que estén /más o menos/ en el rango $-3 < x_j < 3$ y duda si $-\frac{1}{3} < x < \frac{1}{3}$
- Para esto se suele normalizar cada descriptor respecto al rango de sí mismo en la muestra (los m ejemplos de entrada) o respecto a la desviación estándar. Esto se llama */feature scaling/*.
- Otra práctica habitual es centrar en cero los valores, para lo cual se resta la media de la muestra. Esto se llama */mean normalization/*.

**** Más de feature scaling y mean normalization

De la ecuación de actualización de los parámetros de la ecuación de hipótesis
infiero que el vector se mueve _más rápidamente_ en dirección de los parámetros
más grandes. Sin embargo en [[*Reading: Gradient descent in practice I - Feature scaling]] dice:

#+begin_quote
This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.
#+end_quote

****** DONE Averiguar más de esto. ¿Por qué se hace? ¿Tienen que ser de la misma magnitud o ser chicos?
CLOSED: [2020-04-18 sáb 02:46]

- Ver https://www.robertoreif.com/blog/2017/12/21/importance-of-feature-scaling-in-data-modeling-part-2
- Ver https://math.stackexchange.com/questions/2341704/feature-scalings-effect-on-gradient-descent

Estaba entendiendo mal las curvas de nivel. El eje corto de las elipses es el asociado a los descriptores más grandes, con más rango. Son curvas de nivel de $J(\theta)$, no de $J(x)$.  Ahora si estoy de acuerdo.

En regresión lineal (quizás puedo generalizarlo a cualquiera) *los parámetros tienen rangos "inversos" a los de los descriptores que multiplican*. Si un descriptor tiene un rango grande, entonces su parámetro asociado va a tener un rango chico.

_Se podría solucionar también con learning rates diferenciados: más grandes para los descriptores de más rango, más chicos para los de menos rango._

¡Lo que dice en [[*Reading: Gradient descent in practice I - Feature scaling]] está mal expresado entonces!

****** TODO Corregir https://math.stackexchange.com/questions/2341704/feature-scalings-effect-on-gradient-descent

*** Reading: Gradient descent in practice I - Feature scaling


*** Video: Gradient descent in practice II - Learning rate

- Si la función de costo $J(\vec{\theta})$ diverge u oscila, entonces mi tasa de aprendizaje $\alpha$ es muy grande. Si es muy chica, converge lentamente.
- Puedo verlo graficando la función de costo.
- Elegir el valor de $\alpha$ es, a priori, por prueba y error. */¿Habrá heurísticas para determinar un buen valor inicial?/*
- La condición de convergencia también suele depender del problema. Andrew habla de valores absolutos... */¿por qué no usar un $\epsilon$ relativo?/*

*** Reading: Gradient descent in practice II - Learning rate


*** Video: Features and polynomial regression

*** Reading: Features and polynomial regression

- La regresión lineal es ajustar un modelo lineal, de grado 1, una combinación lineal entre las entradas y parámetros.
- Podemos ajustar modelos no lineales como hipótesis si codificamos estas no linealidades dentro de los descriptores. Por ejemplo, para el caso de la estimación de precios de casas, un posible descriptor podría ser el cuadrado del área, y ahí estamos incluyendo algo cuadrático en el modelo.
- Al incluir las no linealidades en los descriptores, pero todavía usando los parámetros como multiplicadores de orden 1, podemos seguir usando el descenso por el gradiente para optimizar.
- Andrew habla también de usar relaciones entre entradas básicas para construir otras entradas. Por ejemplo, el producto de dos descriptores hace un nuevo descriptor que codifica otra relación.

** Computing parameters analitically

*** Video: Normal equation

- Otra forma de optimizar la regresión lineal es resolverla analíticamente con el método de los [[https://en.wikipedia.org/wiki/Least_squares][mínimos cuadrados]] [[https://en.wikipedia.org/wiki/Linear_least_squares][lineales]] / ecuación normal. Esto da la solución óptima (que existe porque hemos dicho que para regresión lineal es un espacio de búsqueda cónvexo con un solo mínimo).

\[  \vec{\theta} = ( X^T \times X )^{-1} \times X^T \times \vec{y}   \]

\[ X = \left[  x^{(i)}  \right]  \]

- A $X$ la llamamos */matriz de diseño/*. Cada fila es un ejemplo, y tiene tamaño $m \times n+1 $

- La complejidad de invertir una matriz es $O(n^3)$ y esto se pone lento para $n > 10^5$. La complejidad del descenso por el gradiente, en cambio, es de $O(k \cdot n^2)$.

- \(( X^T \times X )^{-1} \times X^T = X^{+}\) es la _[[https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse][pseudoinversa]]_ de $X$, y el método de mínimos cuadrados no es más que una solución (óptima en el sentido del error cuadrático) de un sistema de ecuaciones sobredeterminado.

  - La pseudoinversa se puede calcular con /Singular Value Decomposition/ o Descomposición QR, por ejemplo.

  - La regresión por mínimos cuadrados asume muchas cosas que no necesariamente siempre se cumplen. Ver la [[https://en.wikipedia.org/wiki/Robust_regression][regresión robusta]] como alternativa.

*** Reading: Normal equation

*** Video: Normal equation noninvertibility

Si $( X^T \times X )$ no es invertible, entonces puede haber 2 problemas:

1. El sistema esta subdeterminado. Faltan ejemplos, $m < n$ / tenemos muchos descriptores.
   - Después vamos a ver que se soluciona con /regularización/.
2. Algunos descriptores están muy correlacionados / son linealmente dependientes.

Si no es invertible naturalmente (es singular o degenerada) igual se puede invertir con la pseudoinversa. Igual esto no sería problema si hubiésemos usado la pseudoinversa desde un principio en lugar de estar haciéndolo manualmente. Y, nuevamente, seguro hay métodos más robustos (aunque no hay que dejar de hacer análisis de la información con la que contamos).

*** Reading: Normal equation noninvertibility


** Submitting programming assignments

** Review

** Octave/Matlab tutorial

#+BEGIN_SRC octave
  % Para ver una matriz/vector como píxeles con color
  A = magic(9)
  figure
  imagesc(A)
  colorbar
  colormap gray
#+END_SRC


** Review

*** Programming assignment 1: linear regression

- Mi gradient descent convergía pero no al mismo resultado exacto, y más rápida o lentamente. Me faltaba el factor $1/m$.
- Armé una versión vectorizada del gradient descent pero es distinta a la propuesta:

La mía:

- usé $n$ como la longitud de $\theta$, incluyendo los \(1\)s.

#+begin_src octave
  M = length(y); % number of training examples
  N = length(theta);
  error = (X * theta) - y;  % Mx1
  % ponderated_error = repmat(error, [1, N]) .* X;  % MxN
  % ponderated_error = error * ones(1,n) * X  % MxN, equivale al broadcasting
  ponderated_error = error .* X;  % Broadcasting. MxN
  % gradient = sum(ponderated_error,1);  % 1xN
  gradient = ones(1,M) * ponderated_error;  % 1xN, equivalente a la sumatoria
  theta = theta - (alpha/M) * gradient';  % Nx1
#+end_src

\[ \theta_{n \times 1}[i+1] = \theta_{n \times 1}[i] - \frac{\alpha}{m} \left[ 1_{1 \times m} \left( X_{m \times n} \theta_{n \times 1} - Y_{m \times 1} \right)_{m \times 1} 1_{1 \times n} X_{m \times n} \right]^T  \]

La original es más compacta:

\[ \theta_{n \times 1}[i+1] = \theta_{n \times 1}[i] - \frac{\alpha}{m} X_{m \times n}^T (X_{m \times n} \theta_{n \times 1} - Y_{m \times 1} )_{m \times 1} \]

* Semana 3

** Classification and representation

*** Classification

Vamos a ver la *regresión logística* que es un algoritmo de clasificación (aunque su nombre diga /regresión/).

La regresión lineal no es un buen método para la clasificación en variables discretas. Acá necesitamos algo más no lineal. Una opción es usar regresión lineal + un umbral arbitrario de separación, pero aún no es suficiente.

Vamos a ver clasificación binaria. Definimos como $0$ y $1$ a las clases. También usamos *etiqueta* para denominar a la salida $h_\theta(x)$.

*** Hypothesis representation

En clasificación binaria, los resultados observados sólo pueden tomar los valores $0$ y $1$, y por tanto nuestra función de hipótesis debería también sólo tomar esos valores.

Para empezar elegimos una función que esté acotada a ese rango. Una opción es la *función logística* o *sigmoidea*:

\[ h(z) = \frac{1}{1+e^z} \]

\[ h(\theta,x) = h_\theta(x) = \frac{1}{1+e^{\theta^T  x}}\]

- Mapea los reales al intervalo $[0, 1]$.

Podemos interpretar los resultados como la probabilidad de que la hipótesis tome un valor, dada determinada entrada.

- La suma de las probabilidades debe ser $1$.


**** La función logística o sigmoidea

- Se parece a la función cumulativa o función de distribución acumulada de una distribución normal/gaussiana.
  - Pero esta tiene una función explícita, mientras que la FDA de la gaussiana no tiene forma cerrada.
  - La función de densidad de probabilidad asociada "Se parece a la distribución normal en su forma, pero tiene colas más pesadas (y, por lo tanto, menor curtosis)". [[https://es.wikipedia.org/wiki/Distribuci%C3%B3n_log%C3%ADstica][Wikipedia: Distribución logística]]
- Puedo pensar que la FDP de la distribución logística me indica la cantidad de información que me da el valor de un descriptor. En el pico es donde más aporta; luego mientras más me alejo del centro, más claro es que es de una clase o de la otra.
- Es una aproximación suave de la función escalón.

[[file:imgs/002-320px-Logistic-curve.svg.png]]

\[ f(x) = \frac{L}{1+e^{-k(x-x_0)}}  \]

- $L$ es el valor máximo.
- $k$ es la tasa de crecimiento o pendiente de la curva.
- $x_0$ es el centro

*** Decision boundary

La clasificación es discreta; para hacerla discreta necesitamos agregar un umbral a nuestra función de hipótesis. /No entiendo por qué pone el umbral como si fuese una cosa aparte de la función de hipótesis/. Entonces, para la regresión logística hacemos:

\[ y = 0 \quad \text{si} \quad h(z) = h(z(\theta, x)) = h(\theta^T x) \lt 0,5 \]
\[ y = 0 \quad \text{si} \quad h(z) = h(z(\theta, x)) = h(\theta^T x) \geq 0,5 \]

Lo que equivale a

\[ y = 0 \quad \text{si} \quad  \theta^T x < 0,5 \]
\[ y = 1 \quad \text{si} \quad \theta^T x \ge 0,5 \]

La función de entrada a la sigmoidea, $z(\theta,x)$ define el umbral de decisión. Al igual que vimos para regresión lineal, esta función no tiene por qué ser lineal con respecto a los descriptores (/¿mas sí lineal respecto a los parámetros?/), y es la que va a separar las clases en su espacio. Por ejemplo, para dos variables podría ser un elipsoide: \( z(\theta,x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1^2 + \theta_4 x_2^2 \).

** Logistic regression model

*** Cost function

Sea la función de costo $J$ la media de una función de error:

\[ J(\theta) = \frac{1}{m} \sum_1^m  error(\hat{y}, y) \]

Si usamos el error cuadrático medio como función de error para optimizar con el descenso por el gradiente, vamos a tener que derivar una función no lineal. Esto es porque la función logística/sigmoidea $h(z)$ no es lineal con respecto a los parámetros \theta, y por tanto el error cuadrático medio no es una función convexa; esto implica que tiene (¿o puede tener?) más de un mínimo.

Lo que hacemos entonces es proponer otra función de error que sea convexa y diferenciable. Por supuesto, tiene que penalizar las predicciones/hipótesis erróneas. La que se propone es

\[ error(h_\theta(x)) = error(h(\theta,x) = \quad -\log(h_\theta(x)) \quad \text{si} \quad y = 1   \]
\[ error(h_\theta(x)) = error(h(\theta,x) = \quad -\log(1-h_\theta(x)) \quad \text{si} \quad y = 0   \]

#+begin_src octave :exports none
  figure(1, "visible", "off");
  hold on;
  grid
  fplot("-log(x)", [0, 1, 0, 5], 'lineWidth', 4)
  set(gca, "linewidth", 4, "fontsize", 18)
  title('error(h( \theta ,x) = -log(h_\theta(x))')
  legend off;
  % l = legend;
  % set(l, "fontsize", 14, "location","east")
  print("-S300,300", "./imgs/003-01-logcost1.png")
  ans = "[[file:./imgs/003-01-logcost1.png]]"
#+end_src

#+RESULTS:
: [[file:./imgs/003-01-logcost1.png]]

[[file:./imgs/003-01-logcost1.png]]



#+begin_src octave :exports none
  figure(1, "visible", "off");
  hold on;
  grid
  fplot("-log(1-x)", [0, 1, 0, 5], 'lineWidth', 4)
  set(gca, "linewidth", 4, "fontsize", 18)
  title('error(h( \theta ,x) = -log(1-h_\theta(x))')
  legend off;
  % l = legend;
  % set(l, "fontsize", 14, "location","east")
  print("-S300,300", "./imgs/003-02-logcost2.png")
  ans = "[[file:./imgs/003-02-logcost2.png]]"
#+end_src

#+RESULTS:
: [[file:./imgs/003-02-logcost2.png]]

[[file:./imgs/003-02-logcost2.png]]

Nótese que tienden a infinito en $0$ y $1$ respectivamente.

------

En la sección siguiente Andrew dice que esta función de costo (en realidad su forma simplificada) se puede derivar estadísticamente a partir del principio de estimación de máxima verisimilitud.

*** Simplified cost function and gradient descent

**** Forma simplificada

Teníamos a la función de error para la regresión logística como:

\[ error(h_\theta(x)) = error(h(\theta,x)) = \quad -\log(h_\theta(x)) \quad \text{si} \quad y = 1   \]
\[ error(h_\theta(x)) = error(h(\theta,x)) = \quad -\log(1-h_\theta(x)) \quad \text{si} \quad y = 0   \]

La forma simplificada es:

\[ error(h(\theta,x)) = y (-\log(h_\theta(x))) + (1-y) (-\log(1-h_\theta(x)))   \]

\[ error(h(\theta,x)) = -y \log(\hat{y}) - (1-y) \log(\hat{y})  \]

Esta función es convexa (si $h$ es la sigmoidea, al menos).

Luego la función de costo queda:

\[  J(h_\theta(x)) = J(h(\theta,x)) = - \frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))  \right]  \]

La forma vectorizada/matricial es:

\[ J(h(\theta,X)) = -\frac{1}{m} Y^T h(X\theta) - (1-Y)^T (1-h(X\theta))   \]

**** Descenso por el gradiente

Resulta que la derivada $\delta J(\theta,x)/\delta \theta$, es la misma que la que obtuvimos usando el error cuadrático medio (/MSE/) como función de costo para regresión lineal, y entonces la formula de actualización de parámetros es la misma:

\[ \theta_j[n+1] := {\theta}_j[n] - \frac{\alpha}{m}  \sum_{i=1}^m( h_\theta(x^{(i)}) - y^{(i)} ) x_j^{(i)}  \]

En forma vectorizada/matricial:

\[ \theta_{}[i+1] = \theta[i] - \frac{\alpha}{m} X^T (h(X \theta) - Y) \]

*** Advanced optimization

Hay algoritmos generales de optimización mejores (pero más complejos) que el descenso por el gradiente. Andrew nombra:
  - [[https://en.wikipedia.org/wiki/Conjugate_gradient_method][Gradientes conjugados]]
  - BFGS ([[https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm][/Broyden–Fletcher–Goldfarb–Shanno algorithm/]])
  - L-BFGS ([[https://en.wikipedia.org/wiki/Limited-memory_BFGS][/Limited memory BFGS/]])

En Octave tenemos la función ~fminunc~ (de /function minimize unconstrained/) que nos permite optimizar usando una función de costo arbitraria. Le tenemos que proveer esa función de costo, que calcula la función de costo y el gradiente en cada iteración. En el ejemplo de Andrew, la función de costo calcula el gradiente de forma analítica, pero asumo que podrías también tener una memoria y usar diferencias.

** Multiclass classification

*** Multiclass classification: one vs all

Si tenemos $n$ salidas discretas posibles, podemos modelar el problema con $n$ clasificadores binarios, que toman una salida como caso positivo y el resto como negativo.

Una vez que clasificamos con todos los clasificadores, elegimos la salida definitiva como aquella que haya tenido la mayor confianza; y entonces tenemos que ver la probabilidad predicha antes de discretizarla.

Nótese que esto también se cumple en los binarios cuando $n=2$: podemos verlo como que ambos clasificadores definen la misma frontera de decisión.

** Solving the problem of overfitting

*** The problem of overfitting

Empezamos a evaluar la bondad de ajuste de nuestros modelos.

- Un modelo subajustado (/underfitted/) o de alto sesgo (/high bias/) tiene mucho error para los datos con los que se entrenó, y por ende muy probablemente tenga mucho error con entradas nuevas. El modelo no captura las características del espacio del problema.
  - El sesgo se asocia con prejuicio. El modelo prejuzga incorrectamente cómo deberían ser las entradas.
- Un modelo sobreajustado (/overfitted/) predice /demasiado/ correctamente los datos con los que se ajustó, pero no predice correctamente entradas que sean un poco distintas; *no generaliza*. También se habla de que es un modelo con alta varianza (/high variance/), porque el espacio de funciones de hipótesis (de la complejidad propuesta) que predicen bien es muy grande; hay muchos grados de libertad.

En los ejemplos mostrados, el ajuste se incrementa con el grado de las funciones de hipótesis, para regresión lineal. Entonces complejizar las funciones de hipótesis implica agregar más descriptores ---reales o sintéticos---.

**** Opciones para reducir el sobreajuste

Las principales formas de reducir el sobreajuste:

1. Reducir la cantidad de descriptores.
   - Manualmente o con métodos automáticos de selección de modelo.
   - Perdemos información codificada en los descriptores que eliminamos.
2. Usar *regularización*.
   - Mantenemos todos los descriptores pero los ponderamos.

*** Cost function

Introducimos un parámetro de regularización $\lambda$ en la función de costo, que pondera la suma de los cuadrados de los parámetros $\theta$.

- /Creo que este tipo de regularización tiene un nombre/.
- /Usamos el cuadrado para que no se cancelen entre sí y porque es derivable supongo/.

Por ejemplo, para /MSE/:

\[ J(\theta,x,h(x),lambda) =  \frac{1}{2m} \sum_{i=1}^{m} \left[ h(\theta,x^{(i)}) - y^{(i)} \right]^2 + \lambda \sum_{j=1}^{n} \theta_j^2  \]

- Se suele omitir la ordenada al origen, término de sesgo o *intercepto* $\theta_0$ porque no afecta mucho a los resultados.
  - /Me parece que debe haber una razón más interesante, porque esta decisión hace que tengamos que calcular las funciones de costo de forma separada para \(\theta_0\)/.

Lo que buscamos es tener parámetros pequeños, lo que hace que la función de hipótesis sea suave, simple.

Más adelante vamos a ver formas de determinar el valor del parámetro de regularización $\lambda$ para que funcione. Si es muy grande, hay subajuste, y si es muy chico seguimos con sobreajuste.

*** Regularized linear regression

*** Regularized logistic regression

** Review

*** Quiz: Regularization

- Agregar nuevos descriptores nos da una hipótesis igual o mejor a la que tenemos antes de agregarlos, en los datos de entrenamiento/modelado.
  - Asumo que asume convergencia.

*** Programming assignment

* Semana 4
